
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>mytree</title><meta name="generator" content="MATLAB 9.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-01-11"><meta name="DC.source" content="mytree.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">classdef</span> mytree
    <span class="keyword">methods</span>(Static)

        <span class="comment">% Summary</span>

        <span class="comment">% When using a decsion tree to analyse data you do not need to</span>
        <span class="comment">% compress the data by using normalisation or Z-score</span>
        <span class="comment">% standardisation as simple greater than or smaller then tests are</span>
        <span class="comment">% thee basis of the classification process, so refining the data in</span>
        <span class="comment">% this way will have no effect.</span>

        <span class="comment">% Decision trees start with a single node and the data dumped into</span>
        <span class="comment">% this, we then use Axis-parallel spits to seperate examples from</span>
        <span class="comment">% different classes (Think branches of the tree), to split the node</span>
        <span class="comment">% and purify the node have the majority of the data be from one</span>
        <span class="comment">% class, each time the node is split every possible decision is</span>
        <span class="comment">% checked as to what will purify the node the best and then this is</span>
        <span class="comment">% set as a question to ask any new testing examples, if splitting</span>
        <span class="comment">% the node would not further purify the nodes then they should not</span>
        <span class="comment">% be split. Intrestingly though every possible question that can</span>
        <span class="comment">% be asked is assesed some features may never be used if the dont</span>
        <span class="comment">% refine the data well enough (provide any useful information).This</span>
        <span class="comment">% is a recursive function that continues until the node can no</span>
        <span class="comment">% longer be splt (Expanded on below).</span>

        <span class="comment">% Gini's Diversity index is used to identiy how pure a node is on a</span>
        <span class="comment">% scale of 0-1 the closer to zero (the less variation in class lables)</span>
        <span class="comment">% the more pure a node is considered to be.</span>

        <span class="comment">% Restrictions are placed onto the debth a decison tree can grow as</span>
        <span class="comment">% if the become too deep there is a risk of overfitting the</span>
        <span class="comment">% training data, such as having a minimum size limit before</span>
        <span class="comment">% a node is allowed to split. Without restrictions it would be</span>
        <span class="comment">% possible for the tree to box out every example into its own leaf</span>
        <span class="comment">% (Think low k value, KNN)&gt; This also has the same inverse problem</span>
        <span class="comment">% and that with too much restriction we risk underfitting the</span>
        <span class="comment">% data.</span>

        <span class="comment">% When classifying a new testing example, the example is run</span>
        <span class="comment">% through the decision tree unitl it reaches the end of a branch,</span>
        <span class="comment">% (leaf node) testing against each decision made to generate the tree.</span>
        <span class="comment">% it then takes on the most common class label from that node.</span>
        <span class="comment">% Classifying new testing examples will be a fast process, though</span>
        <span class="comment">% building the tree in the training phase could potentially have</span>
        <span class="comment">% massive numbers of calculations required, testing 10 exmples</span>
        <span class="comment">% against a tree that has 500 training examples and contains 10</span>
        <span class="comment">% branch nodes will take between 10-100 comparisons (1-10</span>
        <span class="comment">% comparisons per test)*10 tests, as a leaf node could be at any</span>
        <span class="comment">% point down the tree.</span>

        <span class="keyword">function</span> m = fit(train_examples, train_labels)
            <span class="comment">% The section below defines an empty node that can be reused</span>
            <span class="comment">% to create all of the nodes within the tree.</span>

            <span class="comment">% the unique number to represent the node</span>
            emptyNode.number = [];
            <span class="comment">% training data associated with the node</span>
            emptyNode.examples = [];
            emptyNode.labels = [];
            <span class="comment">% class prediction based on the class labels held by the node</span>
            emptyNode.prediction = [];
            <span class="comment">% a number represining the impurity of the class labels</span>
            <span class="comment">% (Quantitys of different labels)</span>
            emptyNode.impurityMeasure = [];
            <span class="comment">% if node is split then then the information will be divided</span>
            <span class="comment">% and stored into the two child nodes</span>
            emptyNode.children = {};
            <span class="comment">% the infomation that will define the split</span>
            emptyNode.splitFeature = [];
            emptyNode.splitFeatureName = [];
            emptyNode.splitValue = [];

            m.emptyNode = emptyNode;

            <span class="comment">% Initial set up of the model creating an empty node at the</span>
            <span class="comment">% top of the structure then copying all training examples and</span>
            <span class="comment">% labels, generating a class label for the node. This will</span>
            <span class="comment">% classify new data if the node isnt split</span>
            r = emptyNode;
            r.number = 1;
            r.labels = train_labels;
            r.examples = train_examples;
            r.prediction = mode(r.labels);

            <span class="comment">% Further set up parameters including minimum</span>
            <span class="comment">% parent size (number of examples a node must contain before splitting)</span>
            <span class="comment">% class labels and feature labels within each example(column names)</span>
            <span class="comment">% current number of nodes and total number of training exaples</span>
            <span class="comment">% used to train the model</span>
            m.min_parent_size = 10;
            m.unique_classes = unique(r.labels);
            m.feature_names = train_examples.Properties.VariableNames;
			m.nodes = 1;
            m.N = size(train_examples,1);

            <span class="comment">% Now that data has been dumped into the first node this line</span>
            <span class="comment">% calls the trySplit function to generate the tree. This</span>
            <span class="comment">% function tests to see if the node can be split into child</span>
            <span class="comment">% nodes to reduce the impurity within the new nodes, this</span>
            <span class="comment">% fuction is recursive and will only return once it is no</span>
            <span class="comment">% longer able to split any more nodes</span>
            m.tree = mytree.trySplit(m, r);

        <span class="keyword">end</span>

        <span class="keyword">function</span> node = trySplit(m, node)
            <span class="comment">% basic check for minimum size to split(value set earlier)</span>
            <span class="keyword">if</span> size(node.examples, 1) &lt; m.min_parent_size
				<span class="keyword">return</span>
            <span class="keyword">end</span>

            <span class="comment">% Calculate current impurity within  the class labels</span>
            node.impurityMeasure = mytree.weightedImpurity(m, node.labels);

            <span class="comment">% This will check all the different ways of splitting the training data.</span>
            <span class="comment">% It will consider splitting on every feature and unique feature</span>
            <span class="comment">% value of the training data. This will check every possible</span>
            <span class="comment">% division of the class labels and then split with the most "pure data".</span>
            <span class="keyword">for</span> i=1:size(node.examples,2)

				fprintf(<span class="string">'evaluating possible splits on feature %d/%d\n'</span>, i, size(node.examples,2));

				[ps,n] = sortrows(node.examples,i);
                ls = node.labels(n);
                biggest_reduction(i) = -Inf;
                biggest_reduction_index(i) = -1;
                biggest_reduction_value(i) = NaN;
                <span class="keyword">for</span> j=1:(size(ps,1)-1)
                    <span class="keyword">if</span> ps{j,i} == ps{j+1,i}
                        <span class="keyword">continue</span>;
                    <span class="keyword">end</span>

                    this_reduction = node.impurityMeasure - (mytree.weightedImpurity(m, ls(1:j)) + mytree.weightedImpurity(m, ls((j+1):end)));

                    <span class="keyword">if</span> this_reduction &gt; biggest_reduction(i)
                        biggest_reduction(i) = this_reduction;
                        biggest_reduction_index(i) = j;
                    <span class="keyword">end</span>
                <span class="keyword">end</span>

            <span class="keyword">end</span>

            [winning_reduction,winning_feature] = max(biggest_reduction);
            winning_index = biggest_reduction_index(winning_feature);

            <span class="keyword">if</span> winning_reduction &lt;= 0
                <span class="keyword">return</span>
            <span class="keyword">else</span>

                [ps,n] = sortrows(node.examples,winning_feature);
                ls = node.labels(n);

                node.splitFeature = winning_feature;
                node.splitFeatureName = m.feature_names{winning_feature};
                node.splitValue = (ps{winning_index,winning_feature} + ps{winning_index+1,winning_feature}) / 2;

                node.examples = [];
                node.labels = [];
                node.prediction = [];

                node.children{1} = m.emptyNode;
                m.nodes = m.nodes + 1;
                node.children{1}.number = m.nodes;
                node.children{1}.examples = ps(1:winning_index,:);
                node.children{1}.labels = ls(1:winning_index);
                node.children{1}.prediction = mode(node.children{1}.labels);

                node.children{2} = m.emptyNode;
                m.nodes = m.nodes + 1;
                node.children{2}.number = m.nodes;
                node.children{2}.examples = ps((winning_index+1):end,:);
                node.children{2}.labels = ls((winning_index+1):end);
                node.children{2}.prediction = mode(node.children{2}.labels);

                node.children{1} = mytree.trySplit(m, node.children{1});
                node.children{2} = mytree.trySplit(m, node.children{2});
            <span class="keyword">end</span>

        <span class="keyword">end</span>

        <span class="comment">% This uses Gini's diversity index GDI to determine the purity of</span>
        <span class="comment">% the node, this returns a value between 0-1 the closer to 0 the</span>
        <span class="comment">% more pure the node is considered(fewer class labels)</span>
        <span class="keyword">function</span> e = weightedImpurity(m, labels)

            weight = length(labels) / m.N;

            summ = 0;
            obsInThisNode = length(labels);
            <span class="keyword">for</span> i=1:length(m.unique_classes)

				pi = length(labels(labels==m.unique_classes(i))) / obsInThisNode;
                summ = summ + (pi*pi);

			<span class="keyword">end</span>
            g = 1 - summ;

            e = weight * g;

        <span class="keyword">end</span>

        <span class="comment">% Moves down the tree untill it reaches an end and then returns</span>
        <span class="comment">% with the most common class of the node.</span>
        <span class="keyword">function</span> predictions = predict(m, test_examples)

            predictions = categorical;

            <span class="keyword">for</span> i=1:size(test_examples,1)

				fprintf(<span class="string">'classifying example %i/%i\n'</span>, i, size(test_examples,1));
                this_test_example = test_examples{i,:};
                this_prediction = mytree.predict_one(m, this_test_example);
                predictions(end+1) = this_prediction;

			<span class="keyword">end</span>
        <span class="keyword">end</span>

        <span class="keyword">function</span> prediction = predict_one(m, this_test_example)

			node = mytree.descend_tree(m.tree, this_test_example);
            prediction = node.prediction;

        <span class="keyword">end</span>

        <span class="comment">% Tree traversal</span>
        <span class="keyword">function</span> node = descend_tree(node, this_test_example)

			<span class="keyword">if</span> isempty(node.children)
                <span class="keyword">return</span>;
            <span class="keyword">else</span>
                <span class="keyword">if</span> this_test_example(node.splitFeature) &lt; node.splitValue
                    node = mytree.descend_tree(node.children{1}, this_test_example);
                <span class="keyword">else</span>
                    node = mytree.descend_tree(node.children{2}, this_test_example);
                <span class="keyword">end</span>
            <span class="keyword">end</span>

		<span class="keyword">end</span>

        <span class="comment">% describe a tree:</span>
        <span class="keyword">function</span> describeNode(node)

			<span class="keyword">if</span> isempty(node.children)
                fprintf(<span class="string">'Node %d; %s\n'</span>, node.number, node.prediction);
            <span class="keyword">else</span>
                fprintf(<span class="string">'Node %d; if %s &lt;= %f then node %d else node %d\n'</span>, node.number, node.splitFeatureName, node.splitValue, node.children{1}.number, node.children{2}.number);
                mytree.describeNode(node.children{1});
                mytree.describeNode(node.children{2});
            <span class="keyword">end</span>

		<span class="keyword">end</span>

    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">
ans = 

  mytree with no properties.

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018b</a><br></p></div><!--
##### SOURCE BEGIN #####
classdef mytree
    methods(Static)
        
        % Summary
        
        % When using a decsion tree to analyse data you do not need to
        % compress the data by using normalisation or Z-score
        % standardisation as simple greater than or smaller then tests are
        % thee basis of the classification process, so refining the data in
        % this way will have no effect.
        
        % Decision trees start with a single node and the data dumped into
        % this, we then use Axis-parallel spits to seperate examples from
        % different classes (Think branches of the tree), to split the node
        % and purify the node have the majority of the data be from one
        % class, each time the node is split every possible decision is
        % checked as to what will purify the node the best and then this is
        % set as a question to ask any new testing examples, if splitting
        % the node would not further purify the nodes then they should not
        % be split. Intrestingly though every possible question that can 
        % be asked is assesed some features may never be used if the dont 
        % refine the data well enough (provide any useful information).This 
        % is a recursive function that continues until the node can no 
        % longer be splt (Expanded on below). 
                
        % Gini's Diversity index is used to identiy how pure a node is on a
        % scale of 0-1 the closer to zero (the less variation in class lables)
        % the more pure a node is considered to be.
        
        % Restrictions are placed onto the debth a decison tree can grow as
        % if the become too deep there is a risk of overfitting the
        % training data, such as having a minimum size limit before
        % a node is allowed to split. Without restrictions it would be
        % possible for the tree to box out every example into its own leaf
        % (Think low k value, KNN)> This also has the same inverse problem
        % and that with too much restriction we risk underfitting the
        % data.
        
        % When classifying a new testing example, the example is run
        % through the decision tree unitl it reaches the end of a branch,
        % (leaf node) testing against each decision made to generate the tree.
        % it then takes on the most common class label from that node.
        % Classifying new testing examples will be a fast process, though
        % building the tree in the training phase could potentially have
        % massive numbers of calculations required, testing 10 exmples 
        % against a tree that has 500 training examples and contains 10
        % branch nodes will take between 10-100 comparisons (1-10 
        % comparisons per test)*10 tests, as a leaf node could be at any
        % point down the tree.
       
        function m = fit(train_examples, train_labels)
            % The section below defines an empty node that can be reused
            % to create all of the nodes within the tree.
			
            % the unique number to represent the node
            emptyNode.number = [];
            % training data associated with the node
            emptyNode.examples = [];
            emptyNode.labels = [];
            % class prediction based on the class labels held by the node
            emptyNode.prediction = [];
            % a number represining the impurity of the class labels 
            % (Quantitys of different labels) 
            emptyNode.impurityMeasure = [];
            % if node is split then then the information will be divided
            % and stored into the two child nodes
            emptyNode.children = {};
            % the infomation that will define the split
            emptyNode.splitFeature = [];
            emptyNode.splitFeatureName = [];
            emptyNode.splitValue = [];

            m.emptyNode = emptyNode;
            
            % Initial set up of the model creating an empty node at the
            % top of the structure then copying all training examples and
            % labels, generating a class label for the node. This will
            % classify new data if the node isnt split
            r = emptyNode;
            r.number = 1;
            r.labels = train_labels;
            r.examples = train_examples;
            r.prediction = mode(r.labels);
            
            % Further set up parameters including minimum
            % parent size (number of examples a node must contain before splitting)
            % class labels and feature labels within each example(column names)
            % current number of nodes and total number of training exaples
            % used to train the model
            m.min_parent_size = 10;
            m.unique_classes = unique(r.labels);
            m.feature_names = train_examples.Properties.VariableNames;
			m.nodes = 1;
            m.N = size(train_examples,1);
            
            % Now that data has been dumped into the first node this line
            % calls the trySplit function to generate the tree. This
            % function tests to see if the node can be split into child
            % nodes to reduce the impurity within the new nodes, this
            % fuction is recursive and will only return once it is no
            % longer able to split any more nodes
            m.tree = mytree.trySplit(m, r);

        end
        
        function node = trySplit(m, node)
            % basic check for minimum size to split(value set earlier)
            if size(node.examples, 1) < m.min_parent_size
				return
            end
            
            % Calculate current impurity within  the class labels
            node.impurityMeasure = mytree.weightedImpurity(m, node.labels);
            
            % This will check all the different ways of splitting the training data.
            % It will consider splitting on every feature and unique feature
            % value of the training data. This will check every possible
            % division of the class labels and then split with the most "pure data".
            for i=1:size(node.examples,2)

				fprintf('evaluating possible splits on feature %d/%d\n', i, size(node.examples,2));
                
				[ps,n] = sortrows(node.examples,i);
                ls = node.labels(n);
                biggest_reduction(i) = -Inf;
                biggest_reduction_index(i) = -1;
                biggest_reduction_value(i) = NaN;
                for j=1:(size(ps,1)-1)
                    if ps{j,i} == ps{j+1,i}
                        continue;
                    end
                    
                    this_reduction = node.impurityMeasure - (mytree.weightedImpurity(m, ls(1:j)) + mytree.weightedImpurity(m, ls((j+1):end)));
                    
                    if this_reduction > biggest_reduction(i)
                        biggest_reduction(i) = this_reduction;
                        biggest_reduction_index(i) = j;
                    end
                end
				
            end

            [winning_reduction,winning_feature] = max(biggest_reduction);
            winning_index = biggest_reduction_index(winning_feature);

            if winning_reduction <= 0
                return
            else

                [ps,n] = sortrows(node.examples,winning_feature);
                ls = node.labels(n);

                node.splitFeature = winning_feature;
                node.splitFeatureName = m.feature_names{winning_feature};
                node.splitValue = (ps{winning_index,winning_feature} + ps{winning_index+1,winning_feature}) / 2;

                node.examples = [];
                node.labels = []; 
                node.prediction = [];

                node.children{1} = m.emptyNode;
                m.nodes = m.nodes + 1; 
                node.children{1}.number = m.nodes;
                node.children{1}.examples = ps(1:winning_index,:); 
                node.children{1}.labels = ls(1:winning_index);
                node.children{1}.prediction = mode(node.children{1}.labels);
                
                node.children{2} = m.emptyNode;
                m.nodes = m.nodes + 1; 
                node.children{2}.number = m.nodes;
                node.children{2}.examples = ps((winning_index+1):end,:); 
                node.children{2}.labels = ls((winning_index+1):end);
                node.children{2}.prediction = mode(node.children{2}.labels);
                
                node.children{1} = mytree.trySplit(m, node.children{1});
                node.children{2} = mytree.trySplit(m, node.children{2});
            end

        end
        
        % This uses Gini's diversity index GDI to determine the purity of
        % the node, this returns a value between 0-1 the closer to 0 the
        % more pure the node is considered(fewer class labels)
        function e = weightedImpurity(m, labels)

            weight = length(labels) / m.N;

            summ = 0;
            obsInThisNode = length(labels);
            for i=1:length(m.unique_classes)
                
				pi = length(labels(labels==m.unique_classes(i))) / obsInThisNode;
                summ = summ + (pi*pi);
            
			end
            g = 1 - summ;
            
            e = weight * g;

        end
        
        % Moves down the tree untill it reaches an end and then returns
        % with the most common class of the node.
        function predictions = predict(m, test_examples)

            predictions = categorical;
            
            for i=1:size(test_examples,1)
                
				fprintf('classifying example %i/%i\n', i, size(test_examples,1));
                this_test_example = test_examples{i,:};
                this_prediction = mytree.predict_one(m, this_test_example);
                predictions(end+1) = this_prediction;
            
			end
        end

        function prediction = predict_one(m, this_test_example)
            
			node = mytree.descend_tree(m.tree, this_test_example);
            prediction = node.prediction;
        
        end
        
        % Tree traversal
        function node = descend_tree(node, this_test_example)
            
			if isempty(node.children)
                return;
            else
                if this_test_example(node.splitFeature) < node.splitValue
                    node = mytree.descend_tree(node.children{1}, this_test_example);
                else
                    node = mytree.descend_tree(node.children{2}, this_test_example);
                end
            end
        
		end
        
        % describe a tree:
        function describeNode(node)
            
			if isempty(node.children)
                fprintf('Node %d; %s\n', node.number, node.prediction);
            else
                fprintf('Node %d; if %s <= %f then node %d else node %d\n', node.number, node.splitFeatureName, node.splitValue, node.children{1}.number, node.children{2}.number);
                mytree.describeNode(node.children{1});
                mytree.describeNode(node.children{2});        
            end
        
		end
		
    end
end
##### SOURCE END #####
--></body></html>