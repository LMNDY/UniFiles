
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>mynb</title><meta name="generator" content="MATLAB 9.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-01-11"><meta name="DC.source" content="mynb.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">classdef</span> mynb
    <span class="keyword">methods</span>(Static)

        <span class="comment">% Summary</span>

        <span class="comment">% The training data for Naive Bayes uses the normal distribution</span>
        <span class="comment">% (probability density can never be zero, bell curve stretches to</span>
        <span class="comment">% infinity) to model the variation of each feature value (Z score</span>
        <span class="comment">% standardisation is unnessessary) meaning that values far from the</span>
        <span class="comment">% average are unlikly thogh not impossible.</span>
        <span class="comment">% Naive bayes makes the assuption that all features in the dataset</span>
        <span class="comment">% are equally important and independant (class-condiditonal),</span>
        <span class="comment">% we assume a dependancey between class label and feature values.</span>

        <span class="comment">% The training process for Naive Bayes takes the taining data and</span>
        <span class="comment">% works out the probability densities (Expaneded on below) for different</span>
        <span class="comment">% possible classes. Prior probability is calculated by dividing the</span>
        <span class="comment">% number of times that a class occurss in the training data by the</span>
        <span class="comment">% number of training examples.</span>
        <span class="comment">% The size of the training data dosnt effect the</span>
        <span class="comment">% speed of evaluating testing data though will effect the training</span>
        <span class="comment">% phase. Evaluating and multiplying probablity densities will take</span>
        <span class="comment">% the same amount of time reagardless of the amount of training</span>
        <span class="comment">% data.</span>

        <span class="comment">% Testing phase estimates the likelyhood of a new example belonging</span>
        <span class="comment">% to a given class by calculating the probability density for each</span>
        <span class="comment">% feature value separately and then multiplying the answers together.</span>
        <span class="comment">% When testing the current data is compaired to the training data</span>
        <span class="comment">% and a probability that it is the same class as each training example</span>
        <span class="comment">% is calcuculated, the current data is then given the class as the</span>
        <span class="comment">% example with the highest probiblity of being the same.</span>
        <span class="comment">% To classify 10 new testing examples that belong to 5 different</span>
        <span class="comment">% classes with 10 feature values and 100 training examples will</span>
        <span class="comment">% require (((5*10)*10)==((every class*every feat)*every new example</span>
        <span class="comment">% ))) 500 calculations. The</span>
        <span class="comment">% majority of calculations are done at the training phase</span>
        <span class="comment">%(likelyhood*prior= proportional to posterior prob).</span>


        <span class="keyword">function</span> m = fit(train_examples, train_labels)

            <span class="comment">% Finds each unique class label</span>
            m.unique_classes = unique(train_labels);
            <span class="comment">% Finds the number of unique classes</span>
            m.n_classes = length(m.unique_classes);

            m.means = {};
            m.stds = {};

            <span class="comment">% Probability density</span>
            <span class="comment">% i = number of unique classes</span>
            <span class="keyword">for</span> i = 1:m.n_classes

                <span class="comment">% Set up this_class variable</span>
				this_class = m.unique_classes(i);
                <span class="comment">% Finds all the examples of this class and then calculates</span>
                <span class="comment">% the mean and standard deciation of all of the feature</span>
                <span class="comment">% values</span>
                examples_from_this_class = train_examples{train_labels==this_class,:};
                m.means{end+1} = mean(examples_from_this_class);
                m.stds{end+1} = std(examples_from_this_class);

            <span class="keyword">end</span>


            m.priors = [];
            <span class="comment">% A prior is the probibility an estimate of how likely each</span>
            <span class="comment">% class label is to occur, based on how many times it is</span>
            <span class="comment">% present in the training data.</span>

            <span class="keyword">for</span> i = 1:m.n_classes
                <span class="comment">% this loop pulls all training exampes wich belong to the</span>
                <span class="comment">% current class and then calculates the number of examples</span>
                <span class="comment">% from the cuurent class as a fraction of the total, to</span>
                <span class="comment">% give a probability that another randmly chosen will</span>
                <span class="comment">% belong to this class</span>
				this_class = m.unique_classes(i);
                examples_from_this_class = train_examples{train_labels==this_class,:};
                m.priors(end+1) = size(examples_from_this_class,1) / size(train_labels,1);

			<span class="keyword">end</span>

        <span class="keyword">end</span>

        <span class="keyword">function</span> predictions = predict(m, test_examples)

            predictions = categorical;

            <span class="keyword">for</span> i=1:size(test_examples,1)
                <span class="comment">% This loop cycles through all of the test examples and</span>
                <span class="comment">% cals the predict one function</span>
				fprintf(<span class="string">'classifying example %i/%i\n'</span>, i, size(test_examples,1));
                this_test_example = test_examples{i,:};
                this_prediction = mynb.predict_one(m, this_test_example);
                predictions(end+1) = this_prediction;

			<span class="keyword">end</span>
        <span class="keyword">end</span>

        <span class="keyword">function</span> prediction = predict_one(m, this_test_example)
            <span class="comment">% Loops over all the possible class labels then for each,</span>
            <span class="comment">% calculates a likelihood for the current test example given</span>
            <span class="comment">% the class.  It then computes a value which is proportional</span>
            <span class="comment">% to the posterior: the probability of the current class label,</span>
            <span class="comment">% given the test example by multiplying the current calculation</span>
            <span class="comment">% with the prior</span>
            <span class="keyword">for</span> i=1:m.n_classes

				this_likelihood = mynb.calculate_likelihood(m, this_test_example, i);
                this_prior = mynb.get_prior(m, i);
                posterior_(i) = this_likelihood * this_prior;

            <span class="keyword">end</span>
            <span class="comment">% beacuse the values stored in posterior ane proportional to</span>
            <span class="comment">% the true posterieo probablilitys, the most likly class lable</span>
            <span class="comment">% is the wone with the highest probiblity stored this is</span>
            <span class="comment">% accessed using hte max()</span>
            [winning_value_, winning_index] = max(posterior_);
            prediction = m.unique_classes(winning_index);

        <span class="keyword">end</span>

        <span class="comment">% checks the values of every feature in isolation</span>
        <span class="keyword">function</span> likelihood = calculate_likelihood(m, this_test_example, class)

			likelihood = 1;

			<span class="keyword">for</span> i=1:length(this_test_example)
                likelihood = likelihood * mynb.calculate_pd(this_test_example(i), m.means{class}(i), m.stds{class}(i));
            <span class="keyword">end</span>
        <span class="keyword">end</span>

        <span class="keyword">function</span> prior = get_prior(m, class)

			prior = m.priors(class);

		<span class="keyword">end</span>

        <span class="keyword">function</span> pd = calculate_pd(x, mu, sigma)

			first_bit = 1 / sqrt(2*pi*sigma^2);
            second_bit = - ( ((x-mu)^2) / (2*sigma^2) );
            pd = first_bit * exp(second_bit);

		<span class="keyword">end</span>

    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">
ans = 

  mynb with no properties.

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018b</a><br></p></div><!--
##### SOURCE BEGIN #####
classdef mynb
    methods(Static)
        
        % Summary
        
        % The training data for Naive Bayes uses the normal distribution 
        % (probability density can never be zero, bell curve stretches to 
        % infinity) to model the variation of each feature value (Z score 
        % standardisation is unnessessary) meaning that values far from the
        % average are unlikly thogh not impossible.
        % Naive bayes makes the assuption that all features in the dataset 
        % are equally important and independant (class-condiditonal), 
        % we assume a dependancey between class label and feature values. 
        
        % The training process for Naive Bayes takes the taining data and
        % works out the probability densities (Expaneded on below) for different 
        % possible classes. Prior probability is calculated by dividing the
        % number of times that a class occurss in the training data by the
        % number of training examples.
        % The size of the training data dosnt effect the
        % speed of evaluating testing data though will effect the training
        % phase. Evaluating and multiplying probablity densities will take
        % the same amount of time reagardless of the amount of training
        % data.
        
        % Testing phase estimates the likelyhood of a new example belonging
        % to a given class by calculating the probability density for each
        % feature value separately and then multiplying the answers together.
        % When testing the current data is compaired to the training data
        % and a probability that it is the same class as each training example
        % is calcuculated, the current data is then given the class as the
        % example with the highest probiblity of being the same.
        % To classify 10 new testing examples that belong to 5 different
        % classes with 10 feature values and 100 training examples will
        % require (((5*10)*10)==((every class*every feat)*every new example
        % ))) 500 calculations. The
        % majority of calculations are done at the training phase
        %(likelyhood*prior= proportional to posterior prob).
        

        function m = fit(train_examples, train_labels)
            
            % Finds each unique class label
            m.unique_classes = unique(train_labels);
            % Finds the number of unique classes
            m.n_classes = length(m.unique_classes);

            m.means = {};
            m.stds = {};
            
            % Probability density 
            % i = number of unique classes
            for i = 1:m.n_classes
                
                % Set up this_class variable 
				this_class = m.unique_classes(i);
                % Finds all the examples of this class and then calculates
                % the mean and standard deciation of all of the feature
                % values
                examples_from_this_class = train_examples{train_labels==this_class,:};
                m.means{end+1} = mean(examples_from_this_class);
                m.stds{end+1} = std(examples_from_this_class);
            
            end
            

            m.priors = [];
            % A prior is the probibility an estimate of how likely each 
            % class label is to occur, based on how many times it is 
            % present in the training data. 
            
            for i = 1:m.n_classes
                % this loop pulls all training exampes wich belong to the
                % current class and then calculates the number of examples
                % from the cuurent class as a fraction of the total, to
                % give a probability that another randmly chosen will
                % belong to this class
				this_class = m.unique_classes(i);
                examples_from_this_class = train_examples{train_labels==this_class,:};
                m.priors(end+1) = size(examples_from_this_class,1) / size(train_labels,1);
            
			end

        end

        function predictions = predict(m, test_examples)

            predictions = categorical;

            for i=1:size(test_examples,1)
                % This loop cycles through all of the test examples and
                % cals the predict one function
				fprintf('classifying example %i/%i\n', i, size(test_examples,1));
                this_test_example = test_examples{i,:};
                this_prediction = mynb.predict_one(m, this_test_example);
                predictions(end+1) = this_prediction;
            
			end
        end

        function prediction = predict_one(m, this_test_example)
            % Loops over all the possible class labels then for each,
            % calculates a likelihood for the current test example given
            % the class.  It then computes a value which is proportional
            % to the posterior: the probability of the current class label,
            % given the test example by multiplying the current calculation
            % with the prior
            for i=1:m.n_classes

				this_likelihood = mynb.calculate_likelihood(m, this_test_example, i);
                this_prior = mynb.get_prior(m, i);
                posterior_(i) = this_likelihood * this_prior;
            
            end
            % beacuse the values stored in posterior ane proportional to
            % the true posterieo probablilitys, the most likly class lable
            % is the wone with the highest probiblity stored this is
            % accessed using hte max()
            [winning_value_, winning_index] = max(posterior_);
            prediction = m.unique_classes(winning_index);

        end
        
        % checks the values of every feature in isolation
        function likelihood = calculate_likelihood(m, this_test_example, class)
            
			likelihood = 1;
            
			for i=1:length(this_test_example)
                likelihood = likelihood * mynb.calculate_pd(this_test_example(i), m.means{class}(i), m.stds{class}(i));
            end
        end
        
        function prior = get_prior(m, class)
            
			prior = m.priors(class);
        
		end
        
        function pd = calculate_pd(x, mu, sigma)
        
			first_bit = 1 / sqrt(2*pi*sigma^2);
            second_bit = - ( ((x-mu)^2) / (2*sigma^2) );
            pd = first_bit * exp(second_bit);
        
		end
            
    end
end
##### SOURCE END #####
--></body></html>